# DARQN : Agent Pac-Man par Deep Reinforcement Learning

Agent IA jouant Pac-Man avec :
- **CNN** : reconna√Ætre les objets du jeu
- **CBAM** : attention (focus sur ce qui importe)
- **LSTM** : pr√©dire le mouvement des fant√¥mes
- **Dueling DQN** : s√©parer valeur de l'√©tat et avantage des actions

https://github.com/user-attachments/assets/1506fef2-52f8-4ad4-8ea4-0426ab4d2f37

**Versions disponibles :**
- **TensorFlow/Keras** : `DARQN_gym_pacman.py` (version principale)
- **PyTorch** : `DARQN_gym_pacman_pytorch.py` (version exp√©rimentale avec GPU CUDA)

```mermaid
graph LR
    A["Pac-Man<br/>250√ó160 RGB"] 
    B["Preprocessing<br/>84√ó84 Gray"]
    C["Frame Stack<br/>4 frames"]
    D["DARQN<br/>8.9M params"]
    E["Action<br/>Eps-Greedy"]
    F["Reward + Next State"]
    
    A --> B --> C --> D --> E
    E --> F --> D
```

---

## üéÆ Que fait chaque couche du r√©seau

### **Input (84√ó84, 4 canaux)**
- 4 frames empil√©es = montrer le mouvement
- Pac-Man √† droite sur 4 images = l'agent voit la v√©locit√©

### **Conv1 (8√ó8, stride=4) ‚Üí 32 filtres**
- Reconna√Æt les patterns grossiers : murs, pellets, fant√¥mes, Pac-Man
- "O√π sont les objets ?"

### **Conv2 (4√ó4, stride=2) ‚Üí 64 filtres**
- Combine les patterns en situations tactiques
- "Y a-t-il un danger ? Un bonus √† proximit√© ?"

### **Conv3 (3√ó3, stride=1) ‚Üí 128 filtres**
- D√©tails fins, positions pr√©cises
- "O√π exactement sont les choses ?"

### **CBAM : Attention**

**Channel Attention** : "Quelles features importent ?"
- GlobalAvgPool + GlobalMaxPool sur les 128 canaux
- FC layers apprennent les poids
- R√©seau peut ignorer les murs et focus sur les fant√¥mes

**Spatial Attention** : "Quelles positions importent ?"
- Cr√©e une heatmap : "Voil√† o√π l'action se passe"
- Conv2D (7√ó7) apprend les patterns spatiaux

### **LSTM : Dynamiques temporelles**

- LSTM1 (256 units) + LSTM2 (128 units)
- CNN dit o√π sont les choses
- LSTM pr√©dit o√π elles vont
- "Les fant√¥mes tournent √† droite puis √† gauche" = pattern appris

### **Dense Layers : Strat√©gie**

- Combine CNN (spatial) + LSTM (temporal)
- Apprend les d√©cisions : "Ghost √† gauche ET rapide ‚Üí fuis vers le haut"

### **Dueling DQN : Deux flux de d√©cision**

**Value Stream V(s)** : "Cet √©tat est-il bon ?"
- 1 nombre
- √âtat s√ªr = V(s) = +2.5
- √âtat pi√©g√© = V(s) = -1.8

**Advantage Stream A(s,a)** : "Quelle action est meilleure ?"
- 5 nombres (UP, RIGHT, LEFT, DOWN, NOOP)
- UP = +1.0 (escape!), DOWN = -0.5 (vers fant√¥me)

**Formule** : `Q(s,a) = V(s) + [A(s,a) - mean(A)]`

```mermaid
graph TD
    A["Dense 256"]
    B["Value Stream<br/>V(s)"]
    C["Advantage Stream<br/>A(s,a)"]
    D["Dense 128"]
    E["Dense 128"]
    F["V = 1 value"]
    G["A = 5 actions"]
    H["Q = V + A - mean"]
    
    A --> B --> D --> F --> H
    A --> C --> E --> G --> H
```

---

## üèóÔ∏è Architecture

| Couche | Config | Raison |
|--------|--------|--------|
| Conv1 | 8√ó8 stride=4, 32 filtres | Patterns larges |
| Conv2 | 4√ó4 stride=2, 64 filtres | Features mid-level |
| Conv3 | 3√ó3 stride=1, 128 filtres | D√©tails fins |
| CBAM | ratio=8, kernel=7 | Balance capacit√©/computation |
| LSTM1-2 | 256 ‚Üí 128 | Dynamiques temporelles |
| Dense1-2 | 512 ‚Üí 256 | Apprentissage strat√©gique |
| Dueling | Value(1) + Advantage(5) | S√©paration value/action |

---

## üìä Param√®tres du mod√®le

```
Total: 8,905,305 params (33.97 MB)
Trainable: 8,904,857 (99.99%)
Non-trainable: 448 (BatchNorm)
```

---

## üìà R√©sultats d'entra√Ænement

### Version TensorFlow/Keras

#### Progression d'apprentissage

```mermaid
graph LR
    A["√âpisode 1-20<br/>Avg Reward: 15"] 
    B["√âpisode 60-80<br/>Avg Reward: 17"]
    C["√âpisode 100+<br/>Avg Reward: 20"]
    D["√âpisode 500?<br/>Avg Reward: 25-30"]
    
    A -->|Loss ‚Üì| B -->|Loss ‚Üì| C -->|Convergence| D
```

#### Statistiques actuelles (100 √©pisodes)

```
====================================================================================================
Model checkpoint saved at episode 140
Reward -> Best: 101 | Avg (last 20): 18
Loss   -> Avg (last 20): 0.010264
====================================================================================================

Episode: 141/500 | Original: 26 | Custom: 26 | Eps: 0.493 | Time: 75.18s | Steps: 564 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.010111 | Min: 0.000405 | Max: 0.155882 | Trainings: 141
Episode: 142/500 | Original: 28 | Custom: 28 | Eps: 0.491 | Time: 67.59s | Steps: 459 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.009014 | Min: 0.000195 | Max: 0.035159 | Trainings: 114
Episode: 143/500 | Original: 44 | Custom: 44 | Eps: 0.488 | Time: 106.05s | Steps: 786 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.010210 | Min: 0.000892 | Max: 0.154305 | Trainings: 196
Episode: 144/500 | Original: 6 | Custom: 6 | Eps: 0.486 | Time: 37.86s | Steps: 290 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.009773 | Min: 0.000297 | Max: 0.035695 | Trainings: 72
Episode: 145/500 | Original: 21 | Custom: 21 | Eps: 0.483 | Time: 55.18s | Steps: 400 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.011199 | Min: 0.000217 | Max: 0.166430 | Trainings: 100
Episode: 146/500 | Original: 18 | Custom: 18 | Eps: 0.481 | Time: 44.66s | Steps: 382 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.046978 | Min: 0.001104 | Max: 2.505190 | Trainings: 95
Episode: 147/500 | Original: 11 | Custom: 11 | Eps: 0.479 | Time: 39.31s | Steps: 330 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.010871 | Min: 0.000445 | Max: 0.040807 | Trainings: 82
Episode: 148/500 | Original: 16 | Custom: 16 | Eps: 0.476 | Time: 51.91s | Steps: 400 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.011245 | Min: 0.000201 | Max: 0.167474 | Trainings: 100
Episode: 149/500 | Original: 20 | Custom: 20 | Eps: 0.474 | Time: 59.92s | Steps: 460 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.008723 | Min: 0.000253 | Max: 0.034991 | Trainings: 115
Episode: 150/500 | Original: 18 | Custom: 18 | Eps: 0.471 | Time: 56.09s | Steps: 434 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.009429 | Min: 0.000323 | Max: 0.041060 | Trainings: 108
Episode: 151/500 | Original: 19 | Custom: 19 | Eps: 0.469 | Time: 50.96s | Steps: 396 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.008700 | Min: 0.000207 | Max: 0.031896 | Trainings: 99
```

- Loss descend r√©guli√®rement  
- R√©compenses progressent 
- Pas de divergence  
- Peak rewards augmentent

---

### Version PyTorch (Exp√©rimentale)

**Version d√©velopp√©e par curiosit√© pour explorer PyTorch avec GPU CUDA**

#### Record historique apr√®s ~24h d'entra√Ænement

```
Episode: 434/500 | Reward: 328 | Eps: 0.114 | Time: 34.07s | Steps: 700 | Memory: 50000
  ‚îî‚îÄ Loss -> Avg: 0.294509 | Min: 0.025491 | Max: 3.623665 | Trainings: 175
```

**üèÜ Meilleur score obtenu : 328 points**

Cette version PyTorch utilise :
- GPU CUDA acceleration (NVIDIA RTX 4070 SUPER)
- Huber Loss (SmoothL1Loss) pour plus de stabilit√©
- Gradient clipping strict (norm=1.0)
- Reward clipping ([-1, 1])
- Learning rate optimis√© (0.0001)

Le temps d'entra√Ænement moyen par √©pisode est de 15-50 secondes selon la longueur de la partie.


---

## üìä Graphiques d'entra√Ænement

Soon maybe....

### Tableau r√©capitulatif (TensorFlow/Keras)

NOP

### Analyse des r√©compenses

NOP
### Analyse des pertes

NOP
### Progression globale

NOP
### Analyse des √©tapes d'entra√Ænement

NOP

---

## Quick Start

### Version TensorFlow/Keras (principale)

```bash
# Installer les d√©pendances
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Entra√Æner l'agent (train normal - 500 episodes)
python DARQN_gym_pacman.py

# Entra√Æner avec plus d'episodes
python DARQN_gym_pacman.py --episodes 1000

# Reprendre l'entra√Ænement √† partir d'un checkpoint
python DARQN_gym_pacman.py --resume 180

# Reprendre du checkpoint 180 et continuer jusqu'√† l'episode 300
python DARQN_gym_pacman.py --resume 180 --episodes 300

# TensorBoard (optionnel)
tensorboard --logdir=./logs
```

### Version PyTorch (exp√©rimentale)

```bash
# Activer l'environnement virtuel
.venv\Scripts\activate

# Installer PyTorch avec CUDA (pour GPU NVIDIA)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Entra√Æner l'agent avec PyTorch
python DARQN_gym_pacman_pytorch.py

# Reprendre l'entra√Ænement depuis un checkpoint
python DARQN_gym_pacman_pytorch.py --resume 200

# Continuer jusqu'√† 500 episodes
python DARQN_gym_pacman_pytorch.py --resume 200 --episodes 500
```

**Note :** La version PyTorch n√©cessite un GPU NVIDIA compatible CUDA pour de meilleures performances.

---

### Options de ligne de commande

| Argument | Type | D√©faut | Description |
|----------|------|--------|-------------|
| `--resume EPISODE` | int | None | Reprendre depuis episode N (charge `darqn_model_episode_N.weights.h5` ou `.pth`) |
| `--episodes NUM` | int | 500 | Nombre total d'episodes √† entra√Æner |

**Exemples pratiques :**

```bash
# Commencer l'entra√Ænement
python DARQN_gym_pacman.py

# Apr√®s 180 episodes, vous voyez que le mod√®le apprend bien
# Vous pouvez l'interrompre (CTRL+C) et relancer :
python DARQN_gym_pacman.py --resume 180

# Pour continuer jusqu'√† 500 episodes totaux (ou plus)
python DARQN_gym_pacman.py --resume 180 --episodes 500

# Pour ajouter 200 episodes suppl√©mentaires (180 + 200 = 380)
python DARQN_gym_pacman.py --resume 180 --episodes 380
```

---

## üìÅ Structure du projet

```
IPSSI_DARQN/
‚îú‚îÄ‚îÄ DARQN_gym_pacman.py           # Entra√Ænement TensorFlow/Keras
‚îú‚îÄ‚îÄ DARQN_gym_pacman_pytorch.py   # Entra√Ænement PyTorch (exp√©rimental)
‚îú‚îÄ‚îÄ test_darqn_pacman.py          # Tests & √©valuation
‚îú‚îÄ‚îÄ test_pytorch_model.py         # Tests mod√®le PyTorch
‚îú‚îÄ‚îÄ launch_tensorboard.py         # Visualisation temps r√©el
‚îú‚îÄ‚îÄ visualize_metrics.py          # Graphiques d'entra√Ænement
‚îú‚îÄ‚îÄ requirements.txt              # D√©pendances Python
‚îú‚îÄ‚îÄ saved_models/                 # Checkpoints des mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ darqn_model_final.weights.h5      # TensorFlow
‚îÇ   ‚îú‚îÄ‚îÄ darqn_model_episode_20.weights.h5 # TensorFlow
‚îÇ   ‚îú‚îÄ‚îÄ darqn_model_episode_20.pth        # PyTorch
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ metrics/                      # Donn√©es d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ training_metrics.json
‚îÇ   ‚îú‚îÄ‚îÄ training_metrics_detailed.png
‚îÇ   ‚îî‚îÄ‚îÄ training_metrics_combined.png
‚îî‚îÄ‚îÄ logs/                         # TensorBoard logs
```

---

## üß™ Tester le mod√®le entra√Æn√©

```bash
# √âvaluer sur 10 parties
python test_darqn_pacman.py

# Affiche les r√©sultats : score moyen, min, max
```

---

## üìä Algorithme d'entra√Ænement

```mermaid
graph TD
    A["Collect Experience"] --> B["Replay Buffer<br/>50K transitions"]
    B --> C["Sample Batch<br/>32 transitions"]
    C --> D["Compute Q-target<br/>Target Network"]
    C --> E["Compute Q-predicted<br/>Q Network"]
    D --> F["MSE Loss"]
    E --> F
    F --> G["Backprop & Update"]
    G --> H["Every 5 episodes:<br/>Sync Target Network"]
    H -.-> D
```

Hyperparam√®tres cl√©s :
- Learning rate: 0.0001
- Epsilon: 1.0 ‚Üí 0.05 (decay)
- Gamma: 0.99 (futur discount)
- Batch size: 32
- Memory size: 50,000

### Environnement ALE pour Pacman Atari 2600 ROM
https://ale.farama.org/environments/pacman/

---
