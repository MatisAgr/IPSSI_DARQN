{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f110d4b9",
   "metadata": {},
   "source": [
    "# DQN Tetris Training - GPU L4 Version\n",
    "\n",
    "Ce notebook entraîne un agent DQN sur Tetris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b92e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70917f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76578f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU disponible(s): {len(gpus)}\")\n",
    "        print(f\"GPU utilisé: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Aucun GPU détecté, utilisation du CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1537e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Threading (optimisé pour GPU L4)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
    "\n",
    "# Enregistrer les jeux ALE\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4626f7f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e9f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres optimisés pour GPU L4\n",
    "env_name = \"ALE/Tetris-v5\"\n",
    "learning_rate = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99\n",
    "batch_size = 256  # Plus grand batch pour GPU\n",
    "memory_size = 50000\n",
    "episodes = 500\n",
    "update_target_frequency = 10\n",
    "render_mode = None  # Pas de rendu pour GPU\n",
    "loss = 'huber'\n",
    "train_interval = 2  # Entraîner tous les 2 pas\n",
    "\n",
    "# Custom reward system variables\n",
    "one_line_clear_reward = 100         # reward for clearing one line\n",
    "two_lines_clear_reward = 250        # reward for clearing two lines\n",
    "three_lines_clear_reward = 450      # reward for clearing three lines\n",
    "four_lines_clear_reward = 800       # reward for clearing four lines (Tetris!)\n",
    "\n",
    "survival_reward = 1                 # reward per step (survival bonus)\n",
    "game_over_penalty = -200            # penalty for game over\n",
    "\n",
    "positive_reward_multiplier = 10     # multiplier for positive original rewards\n",
    "negative_reward_multiplier = 5      # multiplier for negative original rewards\n",
    "score_increase_divisor = 100        # divisor to estimate lines cleared from score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9442c71",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d788a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser l'environnement Tetris\n",
    "env = gym.make(env_name, render_mode=render_mode)\n",
    "\n",
    "observation_shape = env.observation_space.shape  # (210, 160, 3)\n",
    "action_space = env.action_space.n  # 5 actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f298f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour prétraiter l'observation (image)\n",
    "def preprocess_observation(obs):\n",
    "    \"\"\"Convertir l'image RGB en tableau réduit et normalisé\"\"\"\n",
    "    # Réduire la taille de l'image et convertir en niveaux de gris\n",
    "    obs_gray = np.mean(obs, axis=2)  # Convertir en niveaux de gris\n",
    "    obs_resized = tf.image.resize(obs_gray[..., np.newaxis], (84, 84))\n",
    "    return obs_resized.numpy().flatten().astype(np.float32)\n",
    "\n",
    "state_shape = preprocess_observation(env.reset()[0]).shape[0]\n",
    "action_shape = action_space\n",
    "print(f\"État aplati: {state_shape}\")\n",
    "print(f\"Actions: {action_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677f5a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des callbacks\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=8,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./saved_models/dqn_model_best.weights.h5',\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=0\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='logs',\n",
    "        histogram_freq=0,\n",
    "        write_graph=False,\n",
    "        write_images=False,\n",
    "        update_freq='epoch'\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=0\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9284e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle Q\n",
    "def create_q_model():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(512, activation='relu', input_shape=(state_shape, )),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(action_shape, activation='linear')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        jit_compile=True  # Compilation XLA pour accélérer\n",
    "    )\n",
    "    return model\n",
    "\n",
    "q_model = create_q_model()\n",
    "target_model = create_q_model()\n",
    "target_model.set_weights(q_model.get_weights())\n",
    "\n",
    "print(\"\\nArchitecture du modèle:\")\n",
    "q_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf4697",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la mémoire de replay\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "def store_transition(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d44c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echantillonnage d'un batch de transition\n",
    "def sample_batch():\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    state, action, reward, next_state, done = map(np.asarray, zip(*batch))\n",
    "    return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ef0fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a684ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Politique Epsilon-greedy\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    \"\"\"Politique Epsilon-greedy\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        # Action aléatoire (exploration)\n",
    "        return np.random.choice(action_shape)\n",
    "    else:\n",
    "        # Meilleure action selon le modèle Q (exploitation)\n",
    "        q_values = q_model(state[np.newaxis], training=False)\n",
    "        return np.argmax(q_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea1e41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de récompense personnalisée\n",
    "def calculate_custom_reward(info, prev_info, reward, done):\n",
    "    \"\"\"\n",
    "    Calculates a custom reward based on several game metrics using global variables\n",
    "    \n",
    "    Uses global variables for easy tuning:\n",
    "    - one_line_clear_reward, two_lines_clear_reward, three_lines_clear_reward, four_lines_clear_reward\n",
    "    - survival_reward, game_over_penalty\n",
    "    - positive_reward_multiplier, negative_reward_multiplier\n",
    "    - score_increase_divisor\n",
    "    \"\"\"\n",
    "    custom_reward = 0\n",
    "    \n",
    "    # Massive penalty if game over\n",
    "    if done:\n",
    "        custom_reward += game_over_penalty\n",
    "        return custom_reward\n",
    "    \n",
    "    # Extract information if available\n",
    "    current_score = info.get('score', 0)\n",
    "    prev_score = prev_info.get('score', 0) if prev_info else 0\n",
    "    \n",
    "    # Reward based on score increase\n",
    "    score_increase = current_score - prev_score\n",
    "    \n",
    "    # Detection of completed lines (score usually increases in steps)\n",
    "    if score_increase > 0:\n",
    "        # Estimation of number of lines (adjusted according to game scoring)\n",
    "        lines_cleared = score_increase // score_increase_divisor\n",
    "        \n",
    "        if lines_cleared == 1:\n",
    "            custom_reward += one_line_clear_reward\n",
    "        elif lines_cleared == 2:\n",
    "            custom_reward += two_lines_clear_reward\n",
    "        elif lines_cleared == 3:\n",
    "            custom_reward += three_lines_clear_reward\n",
    "        elif lines_cleared >= 4:\n",
    "            custom_reward += four_lines_clear_reward\n",
    "        else:\n",
    "            custom_reward += score_increase  # Small bonus for other actions\n",
    "    \n",
    "    # survival reward (encourage staying alive)\n",
    "    custom_reward += survival_reward\n",
    "    \n",
    "    # Bonus for positive original reward (successful placement)\n",
    "    if reward > 0:\n",
    "        custom_reward += reward * positive_reward_multiplier\n",
    "    \n",
    "    # Penalty for negative original reward\n",
    "    if reward < 0:\n",
    "        custom_reward += reward * negative_reward_multiplier\n",
    "    \n",
    "    return custom_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22420b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement avec retour de loss\n",
    "def train_step():\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    state, action, reward, next_state, done = sample_batch()\n",
    "\n",
    "    # Forward propagation\n",
    "    next_q_values = target_model(next_state, training=False)\n",
    "    max_next_q_values = np.max(next_q_values, axis=1)\n",
    "\n",
    "    target_q_values = q_model(state, training=False).numpy()\n",
    "    for i, act in enumerate(action):\n",
    "        target_q_values[i][act] = reward[i] if done[i] else reward[i] + gamma * max_next_q_values[i]\n",
    "\n",
    "    # Train with verbose to capture loss\n",
    "    history = q_model.fit(state, target_q_values, verbose=0, callbacks=callbacks, batch_size=batch_size)\n",
    "    \n",
    "    return history.history['loss'][0] if history.history['loss'] else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bf81e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entraînement\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "training_steps_per_episode = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    start = time.time()\n",
    "    obs, info = env.reset()\n",
    "    state = preprocess_observation(obs)\n",
    "    total_reward = 0\n",
    "    total_custom_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    prev_info = info.copy()\n",
    "    episode_losses = []\n",
    "    training_steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "        \n",
    "        # Exécuter l'action dans l'environnement\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "        \n",
    "        # Déterminer si l'épisode est terminé\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Calculer la récompense personnalisée\n",
    "        custom_reward = calculate_custom_reward(info, prev_info, reward, done)\n",
    "        \n",
    "        # Stocker la transition en mémoire avec la récompense personnalisée\n",
    "        store_transition(state, action, custom_reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        total_custom_reward += custom_reward\n",
    "\n",
    "        state = next_state\n",
    "        prev_info = info.copy()\n",
    "        steps += 1\n",
    "        \n",
    "        # Entraîner tous les train_interval pas\n",
    "        if steps % train_interval == 0:\n",
    "            loss = train_step()\n",
    "            if loss is not None:\n",
    "                episode_losses.append(loss)\n",
    "                training_steps += 1\n",
    "\n",
    "    end = time.time()\n",
    "    timelength = end - start\n",
    "        \n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    if episode % update_target_frequency == 0:\n",
    "        target_model.set_weights(q_model.get_weights())\n",
    "    \n",
    "    reward_history.append(total_custom_reward)\n",
    "    training_steps_per_episode.append(training_steps)\n",
    "    \n",
    "    # Calculate episode stats\n",
    "    avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
    "    min_loss = np.min(episode_losses) if episode_losses else 0\n",
    "    max_loss = np.max(episode_losses) if episode_losses else 0\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # Display comprehensive stats\n",
    "    print(f\"Episode: {episode}/{episodes} | Original: {total_reward:.0f} | Custom: {total_custom_reward:.0f} | Epsilon: {epsilon:.3f} | Time: {timelength:.2f}s | Steps: {steps} | Memory: {len(memory)}\")\n",
    "    print(f\"  └─ Loss → Avg: {avg_loss:.6f} | Min: {min_loss:.6f} | Max: {max_loss:.6f} | Trainings: {training_steps}\")\n",
    "\n",
    "    # Sauvegarder le modèle tous les 50 épisodes\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        q_model.save_weights(f'./saved_models/dqn_model_episode_{episode+1}.weights.h5')\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"Model save epoch n {episode+1}\")\n",
    "        print(f\"Reward → Best: {max(reward_history):.0f} | Avg (50 last): {np.mean(reward_history[-50:]):.0f}\")\n",
    "        print(f\"Loss   → Avg (50 last): {np.mean(loss_history[-50:]):.6f}\")\n",
    "        print(f\"{'='*100}\\n\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Sauvegarder le modèle final\n",
    "q_model.save_weights('./saved_models/dqn_model_final.weights.h5')\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Total Episodes: {episodes}\")\n",
    "print(f\"Best Reward: {max(reward_history):.0f}\")\n",
    "print(f\"Average Reward: {np.mean(reward_history):.0f}\")\n",
    "print(f\"Final Average Reward (last 50): {np.mean(reward_history[-50:]):.0f}\")\n",
    "print(f\"Average Loss: {np.mean(loss_history):.6f}\")\n",
    "print(f\"Final Average Loss (last 50): {np.mean(loss_history[-50:]):.6f}\")\n",
    "print(f\"Total Training Steps: {sum(training_steps_per_episode)}\")\n",
    "print(f\"Average Training Steps per Episode: {np.mean(training_steps_per_episode):.2f}\")\n",
    "print(f\"Model saved to: ./saved_models/dqn_model_final.weights.h5\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7421bcf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des résultats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(reward_history)\n",
    "plt.title('Récompenses par épisode')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Récompense totale')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 20\n",
    "moving_avg = np.convolve(reward_history, np.ones(window)/window, mode='valid')\n",
    "plt.plot(moving_avg)\n",
    "plt.title(f'Moyenne mobile (fenêtre={window})')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Récompense moyenne')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/training_progress.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRécompense moyenne: {np.mean(reward_history):.2f}\")\n",
    "print(f\"Récompense maximale: {np.max(reward_history):.2f}\")\n",
    "print(f\"Récompense finale (derniers 20 épisodes): {np.mean(reward_history[-20:]):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
